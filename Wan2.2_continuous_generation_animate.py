import torch
from PIL import Image
from diffsynth.core import load_state_dict
from diffsynth.utils.data import save_video, VideoData
from diffsynth.pipelines.wan_video import WanVideoPipeline, ModelConfig
import time
import numpy as np
import gc

# --- Configuration ---
vram_config = {
    "offload_dtype": torch.bfloat16,
    "offload_device": "cpu",
    "onload_dtype": torch.bfloat16,
    "onload_device": "cuda",
    "preparing_dtype": torch.bfloat16,
    "preparing_device": "cuda",
    "computation_dtype": torch.bfloat16,
    "computation_device": "cuda",
}

# --- Load Pipeline ---
pipe = WanVideoPipeline.from_pretrained(
    torch_dtype=torch.bfloat16,
    device="cuda",
    model_configs=[
        ModelConfig(model_id="Wan-AI/Wan2.2-Animate-14B", origin_file_pattern="diffusion_pytorch_model*.safetensors", **vram_config),
        ModelConfig(model_id="Wan-AI/Wan2.2-Animate-14B", origin_file_pattern="models_t5_umt5-xxl-enc-bf16.pth", **vram_config),
        ModelConfig(model_id="Wan-AI/Wan2.2-Animate-14B", origin_file_pattern="Wan2.1_VAE.pth", **vram_config),
        ModelConfig(model_id="Wan-AI/Wan2.2-Animate-14B", origin_file_pattern="models_clip_open-clip-xlm-roberta-large-vit-huge-14.pth", **vram_config),
    ],
    tokenizer_config=ModelConfig(model_id="Wan-AI/Wan2.1-T2V-1.3B", origin_file_pattern="google/umt5-xxl/"),
)

# Force animate adapter to GPU for speed/stability
pipe.animate_adapter.to("cuda", dtype=torch.bfloat16)
pipe.load_lora(pipe.dit, "<redacted>",alpha=1.1)

# --- Helper Functions ---
def resize_video_frames(video_array, target_size=(1280, 720)):
    resized_frames = []
    for frame in video_array:
        if isinstance(frame, Image.Image):
            img = frame
        else:
            img = Image.fromarray(frame.astype('uint8'))
        img = img.resize(target_size, Image.LANCZOS)
        resized_frames.append(img)
    return resized_frames

# --- Settings ---
target_width, target_height = 1280, 720
gen_frames = 41         # Total frames generated by pipe per segment
input_frames = 37       # Input frames for pose/face (must be n*4+1, e.g., 29*4+1)
                         # This leaves 4 frames "slack" (121 - 117 = 4)

# --- Load Data ---
# 1. Initial Reference Image
first_input_image = Image.open("").resize((target_width, target_height))

# 2. Load FULL source videos
# We load the whole video first so we can slice it inside the loop
print("Loading full source videos...")
raw_pose = VideoData("").raw_data()
raw_face = VideoData("").raw_data()

full_pose_video = resize_video_frames(raw_pose, target_size=(target_width, target_height))
full_face_video = resize_video_frames(raw_face, target_size=(512, 512))

print(f"Total Source Frames Available - Pose: {len(full_pose_video)}, Face: {len(full_face_video)}")

# --- Continuous Generation Loop ---
final_video_frames = []
current_input_image = first_input_image
start_time = time.time()

# Calculate how many segments we can generate based on the shortest video
max_source_len = min(len(full_pose_video), len(full_face_video))
# We step forward by input_frames (117) each time
num_segments = max_source_len // input_frames

print(f"Starting Continuous Generation: {num_segments} segments of {gen_frames} frames each.")
print(f"Note: Input chunk size is {input_frames} (satisfies n*4+1).")

for i in range(num_segments):
    print(f"\n--- Generating Segment {i+1}/{num_segments} ---")
    
    # 1. Determine Slices
    start_idx = i * input_frames
    end_idx = start_idx + input_frames
    
    # Check if we have enough frames left
    if end_idx > max_source_len:
        print("Not enough source frames for a full batch. Stopping.")
        break
        
    current_pose_chunk = full_pose_video[start_idx:end_idx]
    current_face_chunk = full_face_video[start_idx:end_idx]
    
    print(f"Processing source frames: {start_idx} to {end_idx} (Length: {len(current_pose_chunk)})")

    # 2. Run Pipeline
    # Use 'current_input_image' which is updated at the end of the loop
    segment_video = pipe(
        prompt="视频中的人在做动作",
        seed=0, 
        tiled=True,
        input_image=current_input_image,
        animate_pose_video=current_pose_chunk,
        animate_face_video=current_face_chunk,
        num_frames=gen_frames, 
        height=target_height, 
        width=target_width,
        num_inference_steps=20, 
        cfg_scale=1,
    )
    
    # 3. Stitching Logic
    if i == 0:
        # First segment: Keep all frames
        final_video_frames.extend(segment_video)
    else:
        # Subsequent segments:
        # To make it seamless, usually the first frame of the new segment 
        # is very similar/identical to the last frame of the previous segment (the input_image).
        # We skip the first frame to avoid a 'stutter' (duplicate frame).
        final_video_frames.extend(segment_video[1:])
    
    # 4. Update Context for Next Loop
    # The last frame of this generation becomes the seed image for the next
    current_input_image = segment_video[-1]
    
    # 5. Clean up VRAM
    # Essential for continuous generation to avoid fragmentation/OOM
    del segment_video, current_pose_chunk, current_face_chunk
    torch.cuda.empty_cache()
    gc.collect()

# --- Save Final Result ---
print(f"\nGeneration Complete!")
print(f"Total Generation time: {(time.time() - start_time):.2f} seconds")
print(f"Total Frames: {len(final_video_frames)}")

save_video(final_video_frames, ".mp4", fps=24, quality=5)
print("Video saved to .mp4")
